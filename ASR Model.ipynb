{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deepspeech 1** Implementation referenced from arXiv:1412.5567. All credits to original authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import sklearn\n",
    "import tensorflow\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "# print(\"Finished importing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default params for DeepSpeech model\n",
    "def buildModel_test(input_dim, output_dim, context = 5, units = 1024, dropouts = (0.1,0.1,0)):    \n",
    "    \n",
    "    # Create Input Layer and preprocessing for first FC layer\n",
    "    _input = layers.Input([None, input_dim])\n",
    "    \n",
    "    # Call Keras expand_dims to add extra channel dimension (axis = -1) to input required by convolution 2D layer\n",
    "    x = layers.Lambda(keras.backend.expand_dims, arguments = dict(axis=-1))(_input)\n",
    "    \n",
    "    # ** Layer 1 **\n",
    "    # A zero-padded convolutional layer applied on time dimension only.\n",
    "    # Thus, we will need to pad time dimension and specify kernel size for time dimension based on specified context.\n",
    "    x = layers.ZeroPadding2D(padding=(context,0))(x)\n",
    "    x = layers.Conv2D(filters = units, kernel_size=(context*2+1, input_dim))(x)\n",
    "    \n",
    "    # Reshaping after convolution\n",
    "    x = layers.Lambda(keras.backend.squeeze, arguments=dict(axis=2))(x)\n",
    "    \n",
    "    # Clipped Relu (max=20) and Dropout are then applied to convolutional output:\n",
    "    x = layers.ReLU(max_value=20)(x)\n",
    "    x = layers.Dropout(rate=dropouts[0])(x)\n",
    "    \n",
    "    # ** Layer 2 **\n",
    "    # Dense Layer, followed by clipped Relu and Dropout operating on \n",
    "    # independent data for each time-step via TimeDistributed Layer\n",
    "    x = layers.TimeDistributed(layers.Dense(units))(x)\n",
    "    x = layers.ReLU(max_value=20)(x)\n",
    "    x = layers.Dropout(rate=dropouts[1])(x)\n",
    "    \n",
    "    # ** Layer 3 **\n",
    "    # Similar to Layer 2\n",
    "    x = layers.TimeDistributed(layers.Dense(units))(x)\n",
    "    x = layers.ReLU(max_value=20)(x)\n",
    "    x = layers.Dropout(rate=dropouts[2])(x)\n",
    "    \n",
    "    # ** Layer 4 **\n",
    "    # Bidirectional RNN, with output being sum of both forward and backward units\n",
    "    x = layers.Bidirectional(layers.SimpleRNN(units, return_sequences=True), merge_mode='sum')(x)\n",
    "    \n",
    "    # ** Layer 5 **\n",
    "    # Final Dense Layer followed by Softmax to get predictions along characters for each timestep\n",
    "    x = layers.TimeDistributed(layers.Dense(output_dim))(x)\n",
    "    _output = layers.Softmax()(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(_input, _output)\n",
    "                               \n",
    "   # Print summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(input_dim, output_dim, context = 5, units = 1024, dropouts = (0.1,0.1,0)):    \n",
    "    model2 = keras.Sequential()\n",
    "    # Create Input Layer and preprocessing for first FC layer\n",
    "    model2.add(layers.Input([None, input_dim]))\n",
    "    \n",
    "    # Call Keras expand_dims to add extra channel dimension (axis = -1) to input required by convolution 2D layer\n",
    "    model2.add(layers.Lambda(keras.backend.expand_dims, arguments = dict(axis=-1)))\n",
    "    \n",
    "    # ** Layer 1 **\n",
    "    # A zero-padded convolutional layer applied on time dimension only.\n",
    "    # Thus, we will need to pad time dimension and specify kernel size for time dimension based on specified context.\n",
    "    model2.add(layers.ZeroPadding2D(padding=(context,0)))\n",
    "    model2.add(layers.Conv2D(filters = units, kernel_size=(context*2+1, input_dim)))\n",
    "    \n",
    "    # Reshaping after convolution\n",
    "    model2.add(layers.Lambda(keras.backend.squeeze, arguments=dict(axis=2)))\n",
    "    \n",
    "    # Clipped Relu (max=20) and Dropout are then applied to convolutional output:\n",
    "    model2.add(layers.ReLU(max_value=20))\n",
    "    model2.add(layers.Dropout(rate=dropouts[0]))\n",
    "    \n",
    "    # ** Layer 2 **\n",
    "    # Dense Layer, followed by clipped Relu and Dropout operating on \n",
    "    # independent data for each time-step via TimeDistributed Layer\n",
    "    model2.add(layers.TimeDistributed(layers.Dense(units)))\n",
    "    model2.add(layers.ReLU(max_value=20))\n",
    "    model2.add(layers.Dropout(rate=dropouts[1]))\n",
    "    \n",
    "    # ** Layer 3 **\n",
    "    # Similar to Layer 2\n",
    "    model2.add(layers.TimeDistributed(layers.Dense(units)))\n",
    "    model2.add(layers.ReLU(max_value=20))\n",
    "    model2.add(layers.Dropout(rate=dropouts[2]))\n",
    "    \n",
    "    # ** Layer 4 **\n",
    "    # Bidirectional RNN, with output being sum of both forward and backward units\n",
    "    model2.add(layers.Bidirectional(layers.SimpleRNN(units, return_sequences=True), merge_mode='sum'))\n",
    "    model2.add(layers.TimeDistributed(layers.Dense(output_dim, activation='softmax')))\n",
    "    \n",
    "    model2.summary()\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_48 (Lambda)           (None, None, 1000, 1)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPaddi (None, None, 1000, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, None, 1, 1024)     11265024  \n",
      "_________________________________________________________________\n",
      "lambda_49 (Lambda)           (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "re_lu_50 (ReLU)              (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_43 (TimeDis (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "re_lu_51 (ReLU)              (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "re_lu_52 (ReLU)              (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, None, 1024)        4196352   \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, None, 25)          25625     \n",
      "=================================================================\n",
      "Total params: 17,586,201\n",
      "Trainable params: 17,586,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, None, 1000)]      0         \n",
      "_________________________________________________________________\n",
      "lambda_50 (Lambda)           (None, None, 1000, 1)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPaddi (None, None, 1000, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, None, 1, 1024)     11265024  \n",
      "_________________________________________________________________\n",
      "lambda_51 (Lambda)           (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "re_lu_53 (ReLU)              (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "re_lu_54 (ReLU)              (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_47 (TimeDis (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "re_lu_55 (ReLU)              (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, None, 1024)        4196352   \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, None, 25)          25625     \n",
      "_________________________________________________________________\n",
      "softmax_8 (Softmax)          (None, None, 25)          0         \n",
      "=================================================================\n",
      "Total params: 17,586,201\n",
      "Trainable params: 17,586,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Test build and summary on random input/output size to make sure equivalent\n",
    "model = buildModel(1000, 25)\n",
    "model = buildModel_test(1000, 25)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# TODO:\n",
    "    # Implement CTC Loss\n",
    "    # Check details for Nesterov Accelerated optimizer to pass into compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
